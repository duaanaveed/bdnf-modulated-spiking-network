{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee55e6c-2aa0-4804-9821-bec1d29b692d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import imageio.v3 as iio\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "# Shared config and IO helpers (used by training + validation cells)\n",
    "CLASS_NAMES = (\"circle\", \"square\", \"triangle\")\n",
    "TRAIN_ROOT = Path(\"/Users/duaanaveed/Downloads/archive/training/shapes/train\")\n",
    "VAL_ROOT = Path(\"/Users/duaanaveed/Downloads/archive/shapes/valid\")\n",
    "\n",
    "def normalise01(img):\n",
    "    \"\"\"Convert image to floats in [0,1] range (supports grayscale/RGB).\"\"\"\n",
    "    img = img.astype(float, copy=False)\n",
    "    if img.ndim == 3:\n",
    "        img = img.mean(axis=-1)\n",
    "    if img.max() > 1.0:\n",
    "        img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def list_images_with_labels(root: Path, class_names):\n",
    "    \"\"\"List image paths and integer labels given a root folder and class list.\"\"\"\n",
    "    paths, labels = [], []\n",
    "    for cls in class_names:\n",
    "        folder = root / cls\n",
    "        for f in sorted(folder.glob(\"*.png\")):\n",
    "            paths.append(f)\n",
    "            labels.append(class_names.index(cls))\n",
    "    return paths, np.array(labels, int)\n",
    "\n",
    "def load_images(root: Path, class_names):\n",
    "    \"\"\"Load and normalise images; return numpy array and labels.\"\"\"\n",
    "    paths, labels = list_images_with_labels(root, class_names)\n",
    "    imgs = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            img = normalise01(iio.imread(p))\n",
    "            imgs.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped {p.name}: {e}\")\n",
    "    return np.stack(imgs), labels, paths\n",
    "\n",
    "# Setup output directory\n",
    "directory = \"TESTSTDP_medium_epoh_colour_test\"\n",
    "Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load training images (shared loader)\n",
    "imgs, labels, train_paths = load_images(TRAIN_ROOT, CLASS_NAMES)\n",
    "print(f\"Loaded {len(imgs)} training images of shape {imgs[0].shape}\")\n",
    "\n",
    "H, W = imgs[0].shape\n",
    "Npix = H * W\n",
    "\n",
    "# Simulation parameters\n",
    "defaultclock.dt = 1*ms\n",
    "Ton = 300*ms      # Stimulus presentation duration\n",
    "Tisi = 50*ms      # Inter-stimulus interval\n",
    "epochs = 5\n",
    "max_rate = 10*Hz  # Maximum input firing rate\n",
    "max_rate_scalar = float(max_rate/Hz)\n",
    "\n",
    "# Precompute Poisson rates for all training images (keeps Brian2 units)\n",
    "train_rates = (1.0 - imgs.reshape(len(imgs), -1)) * max_rate\n",
    "\n",
    "def img_to_rates_np(img):\n",
    "    \"\"\"Convert image to firing-rate vector (unitless, for numpy-only ops).\"\"\"\n",
    "    return (1.0 - img.reshape(-1)) * max_rate_scalar\n",
    "\n",
    "# Input layer: Poisson neurons encoding pixel intensities\n",
    "IN = PoissonGroup(Npix, rates=0*Hz)\n",
    "\n",
    "# Output layer parameters\n",
    "tau_m = 20*ms     # Membrane time constant\n",
    "v_rest = 0*mV     # Resting potential\n",
    "v_thr = 4*mV      # Spike threshold\n",
    "v_reset = 0*mV    # Reset potential\n",
    "tau_e = 5*ms      # Excitatory current decay\n",
    "tau_i = 10*ms     # Inhibitory current decay\n",
    "\n",
    "# Output layer: 3 LIF neurons with teacher signal\n",
    "eqs_out = '''\n",
    "dv/dt  = (-(v - v_rest) + ge - gi + Iteach) / tau_m : volt\n",
    "dge/dt = -ge/tau_e : volt\n",
    "dgi/dt = -gi/tau_i : volt\n",
    "Iteach : volt\n",
    "s : integer\n",
    "'''\n",
    "\n",
    "OUT = NeuronGroup(\n",
    "    N=3, model=eqs_out,\n",
    "    threshold='v>v_thr',\n",
    "    reset='v=v_reset; s += 1',\n",
    "    refractory=6*ms,\n",
    "    method='euler'\n",
    ")\n",
    "\n",
    "OUT.v = v_rest\n",
    "if hasattr(OUT, 'ge'): OUT.ge = 0\n",
    "if hasattr(OUT, 'gi'): OUT.gi = 0\n",
    "\n",
    "# BDNF profiles for modulating plasticity\n",
    "BDNF_PROFILES = {\n",
    "    \"low\": {\n",
    "        \"eta_scale\": 0.5,\n",
    "        \"ltp_scale\": 0.8,\n",
    "        \"ltd_scale\": 1.1,\n",
    "        \"norm_every\": 1,\n",
    "    },\n",
    "    \"medium\": {\n",
    "        \"eta_scale\": 1.0,\n",
    "        \"ltp_scale\": 1.0,\n",
    "        \"ltd_scale\": 1.0,\n",
    "        \"norm_every\": 2,\n",
    "    },\n",
    "    \"high\": {\n",
    "        \"eta_scale\": 1.5,\n",
    "        \"ltp_scale\": 1.2,\n",
    "        \"ltd_scale\": 0.9,\n",
    "        \"norm_every\": 3,\n",
    "    },\n",
    "    \"high_homeostatic\": {\n",
    "        \"eta_scale\": 1.3,\n",
    "        \"ltp_scale\": 1.1,\n",
    "        \"ltd_scale\": 0.95,\n",
    "        \"norm_every\": 3,\n",
    "    },\n",
    "}\n",
    "\n",
    "BDNF_LEVEL = \"medium\"\n",
    "\n",
    "# Base STDP parameters\n",
    "eta_base = 0.02\n",
    "Apre_base = 0.015\n",
    "Apost_base = -0.016\n",
    "\n",
    "# Apply BDNF profile scaling\n",
    "_bd = BDNF_PROFILES[BDNF_LEVEL]\n",
    "eta = eta_base * _bd[\"eta_scale\"]\n",
    "Apre = Apre_base * _bd[\"ltp_scale\"]\n",
    "Apost = Apost_base * _bd[\"ltd_scale\"]\n",
    "norm_every = _bd[\"norm_every\"]\n",
    "\n",
    "print(f\"[BDNF={BDNF_LEVEL}] eta={eta:.4f}, Apre={Apre:.4f}, Apost={Apost:.4f}, norm_every={norm_every}\")\n",
    "\n",
    "# STDP parameters\n",
    "taupre = 10*ms\n",
    "taupost = 10*ms\n",
    "wmax = 0.05\n",
    "wscale = 5*mV\n",
    "\n",
    "# Plasticity switch (1.0 during training, 0.0 during testing)\n",
    "plasticity = 1.0\n",
    "\n",
    "# Synapses with STDP\n",
    "S = Synapses(IN, OUT,\n",
    "    model='''\n",
    "        w : 1\n",
    "        dapre/dt  = -apre/taupre  : 1 (event-driven)\n",
    "        dapost/dt = -apost/taupost : 1 (event-driven)\n",
    "    ''',\n",
    "    on_pre='''\n",
    "        ge_post += w * wscale\n",
    "        apre += Apre\n",
    "        w = clip(w + plasticity*eta*apost, 0, wmax)\n",
    "    ''',\n",
    "    on_post='''\n",
    "        apost += Apost\n",
    "        w = clip(w + plasticity*eta*apre, 0, wmax)\n",
    "    ''',\n",
    "    method='euler'\n",
    ")\n",
    "\n",
    "# Full connectivity with random initialisation\n",
    "S.connect(True)\n",
    "S.w = '0.002*rand()'\n",
    "S.apre = 0\n",
    "S.apost = 0\n",
    "\n",
    "# Lateral inhibition between output neurons\n",
    "ginh = 0.5*mV\n",
    "Sinhib = Synapses(OUT, OUT, on_pre='gi_post += ginh')\n",
    "Sinhib.connect(condition='i != j')\n",
    "Sinhib.delay = 1*ms\n",
    "\n",
    "# Spike monitors\n",
    "spike_inp = SpikeMonitor(IN)\n",
    "spike_out = SpikeMonitor(OUT)\n",
    "\n",
    "# Training loop\n",
    "net = Network(collect())\n",
    "teacher_amp = 6*mV\n",
    "\n",
    "correct = 0\n",
    "seen = 0\n",
    "\n",
    "for ep in range(epochs):\n",
    "    order = np.random.permutation(len(imgs))\n",
    "    for k in order:\n",
    "        label_scalar = int(labels[k])\n",
    "        \n",
    "        # Set input firing rates (precomputed)\n",
    "        IN.rates = train_rates[k]\n",
    "        \n",
    "        # Apply teacher signal to correct output neuron\n",
    "        OUT.Iteach = 0*volt\n",
    "        OUT.Iteach[label_scalar] = teacher_amp\n",
    "        OUT.s = 0\n",
    "        \n",
    "        # Present stimulus\n",
    "        net.run(Ton)\n",
    "        \n",
    "        # Make prediction based on spike counts\n",
    "        counts = OUT.s[:]\n",
    "        pred = int(np.argmax(counts))\n",
    "        is_ok = (pred == label_scalar)\n",
    "        correct += int(is_ok)\n",
    "        seen += 1\n",
    "        print(f\"img {k:02d} label={labels[k]} counts={counts} → pred={pred} {'✓' if is_ok else '✗'}\")\n",
    "        \n",
    "        # Inter-stimulus interval: reset activity\n",
    "        IN.rates = 0*Hz\n",
    "        OUT.Iteach[:] = 0*mV\n",
    "        OUT.v = v_rest\n",
    "        OUT.ge = 0*mV\n",
    "        OUT.gi = 0*mV\n",
    "        S.apre = 0\n",
    "        S.apost = 0\n",
    "        net.run(Tisi)\n",
    "    \n",
    "    # Epoch summary\n",
    "    print(f\"[epoch {ep+1}] running accuracy: {correct}/{seen} = {100*correct/seen:.1f}%\")\n",
    "    \n",
    "    # Periodic weight normalisation\n",
    "    if (ep + 1) % norm_every == 0:\n",
    "        i = np.array(S.i[:])\n",
    "        j = np.array(S.j[:])\n",
    "        for out_idx in np.unique(j):\n",
    "            idx = np.where(j == out_idx)[0]\n",
    "            total = float(np.sum(S.w[idx]))\n",
    "            if total > 0:\n",
    "                S.w[idx] *= (1.0 / total)**0.5\n",
    "        print(f\"[BDNF={BDNF_LEVEL}] soft-normalised incoming weights\")\n",
    "\n",
    "# Disable plasticity after training\n",
    "plasticity = 0.0\n",
    "\n",
    "# Build weight matrix for visualisation\n",
    "n_post = len(np.unique(S.j[:]))\n",
    "n_pre = len(np.unique(S.i[:]))\n",
    "print(f\"n_pre = {n_pre}, n_post = {n_post}, total weights = {len(S.w[:])}\")\n",
    "\n",
    "Wmat = np.zeros((n_pre, n_post))\n",
    "for i, j, w in zip(S.i[:], S.j[:], S.w[:]):\n",
    "    Wmat[i, j] = w\n",
    "\n",
    "# Save receptive field visualisations\n",
    "for k in range(n_post):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(Wmat[:, k].reshape(H, W))\n",
    "    ax.set_title(f'OUT{k} receptive field')\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    out_png = Path(directory) / f\"rf_OUT{k}.png\"\n",
    "    fig.savefig(out_png, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"saved: {out_png}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# Save trained weights\n",
    "def save_receptive_fields(S, H, W, path=\"trained_receptive_fields_medium.npz\"):\n",
    "    \"\"\"Save trained synaptic weights to disk.\"\"\"\n",
    "    n_post = len(np.unique(S.j[:]))\n",
    "    n_pre = H * W\n",
    "    \n",
    "    Wdense = np.zeros((n_pre, n_post), dtype=np.float32)\n",
    "    Wdense[np.asarray(S.i[:]), np.asarray(S.j[:])] = np.asarray(S.w[:], dtype=np.float32)\n",
    "    \n",
    "    np.savez_compressed(\n",
    "        path,\n",
    "        W=Wdense,\n",
    "        H=np.int32(H),\n",
    "        WIMG=np.int32(W),\n",
    "        n_pre=np.int32(n_pre),\n",
    "        n_post=np.int32(n_post),\n",
    "        bdnf_level=np.array(BDNF_LEVEL, dtype=object),\n",
    "    )\n",
    "    print(f\"[saved] {path}  shape={Wdense.shape}\")\n",
    "\n",
    "save_receptive_fields(S, H, W, \"trained_receptive_fields_medium.npz\")\n",
    "\n",
    "# Cleanup\n",
    "del imgs, train_paths\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed2458-432a-4641-9185-1f73cb7f4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import imageio.v3 as iio\n",
    "\n",
    "# Configuration (reuse shared helpers from first cell)\n",
    "CLASS_NAMES = (\"circle\", \"square\", \"triangle\")\n",
    "VAL_ROOT = Path(\"/Users/duaanaveed/Downloads/archive/shapes/valid\")\n",
    "use_norm = True\n",
    "\n",
    "# Load trained receptive fields\n",
    "bundle = np.load(\"trained_receptive_fields_medium.npz\")\n",
    "weights = bundle[\"W\"]\n",
    "H = int(bundle[\"H\"])\n",
    "W = int(bundle[\"WIMG\"])\n",
    "\n",
    "# Load validation dataset with shared loader\n",
    "paths, y_true = list_images_with_labels(VAL_ROOT, list(CLASS_NAMES))\n",
    "X = []\n",
    "for p in paths:\n",
    "    im = normalise01(iio.imread(p))\n",
    "    if im.shape != (H, W):\n",
    "        raise ValueError(f\"{p.name} has {im.shape}, expected {(H, W)}\")\n",
    "    X.append(im)\n",
    "X = np.stack(X)\n",
    "print(f\"Loaded {len(X)} validation images of shape {X[0].shape}\")\n",
    "\n",
    "# Precompute rate-encoded validation vectors (unitless for numpy ops)\n",
    "val_rates = (1.0 - X.reshape(len(X), -1)) * max_rate_scalar\n",
    "\n",
    "# Normalise receptive field weights if enabled\n",
    "Wuse = weights / (np.linalg.norm(weights, axis=0, keepdims=True) + 1e-12) if use_norm else weights\n",
    "\n",
    "# Compute projection scores and predictions\n",
    "scores = val_rates @ Wuse\n",
    "y_pred = scores.argmax(1)\n",
    "margins = scores.max(1) - np.partition(scores, -2, axis=1)[:, -2]\n",
    "\n",
    "# Evaluation metrics\n",
    "acc = (y_pred == y_true).mean()\n",
    "cm = np.zeros((len(CLASS_NAMES), len(CLASS_NAMES)), int)\n",
    "for t, p in zip(y_true, y_pred):\n",
    "    cm[t, p] += 1\n",
    "\n",
    "print(f\"Template accuracy (use_norm={use_norm}): {100*acc:.1f}%\")\n",
    "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902eef67-1304-4ef6-b089-b8d15b71a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def contribution_map(idx, class_idx):\n",
    "    \"\"\"Compute pixel-wise contribution to a specific class prediction.\"\"\"\n",
    "    rates = val_rates[idx]\n",
    "    w = Wuse[:, class_idx]\n",
    "    return (rates * w).reshape(H, W)\n",
    "\n",
    "def cosine_scores(idx):\n",
    "    \"\"\"Compute cosine similarity between one image and each receptive field.\"\"\"\n",
    "    v = val_rates[idx]\n",
    "    v = v / (np.linalg.norm(v) + 1e-12)\n",
    "    Wc = Wuse / (np.linalg.norm(Wuse, axis=0, keepdims=True) + 1e-12)\n",
    "    return v @ Wc\n",
    "\n",
    "# Visualise single example\n",
    "k = 40  # Index to examine different images\n",
    "im = X[k]\n",
    "true_lbl = CLASS_NAMES[y_true[k]]\n",
    "pred_lbl = CLASS_NAMES[y_pred[k]]\n",
    "cos = cosine_scores(k)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "axes[0].imshow(im, cmap='gray')\n",
    "axes[0].set_title(f\"Image\\ntrue={true_lbl}\\npred={pred_lbl}\")\n",
    "\n",
    "for c in range(len(CLASS_NAMES)):\n",
    "    axes[1+c].imshow(contribution_map(k, c), cmap='magma')\n",
    "    axes[1+c].set_title(f\"{CLASS_NAMES[c]}\\ncos={cos[c]:.3f}\\nscore={scores[k,c]:.1f}\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f0da82-25ad-4105-9746-442c20934518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Find hardest and easiest examples by margin\n",
    "idx_sorted = np.argsort(margins)\n",
    "hard = idx_sorted[:6]  # Lowest margins (hardest)\n",
    "easy = idx_sorted[-6:]  # Highest margins (easiest)\n",
    "\n",
    "def show_panel(idxs, title):\n",
    "    \"\"\"Display a panel of images with their contribution maps.\"\"\"\n",
    "    n = len(idxs)\n",
    "    fig, axes = plt.subplots(n, 1+len(CLASS_NAMES), figsize=(12, 2.5*n))\n",
    "    if n == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "    \n",
    "    for row, k in enumerate(idxs):\n",
    "        im = X[k]\n",
    "        t = y_true[k]\n",
    "        p = y_pred[k]\n",
    "        cos = cosine_scores(k)\n",
    "        \n",
    "        # Show original image\n",
    "        axes[row, 0].imshow(im, cmap='gray')\n",
    "        axes[row, 0].set_title(\n",
    "            f\"{title}\\n{paths[k].name}\\n\"\n",
    "            f\"true={CLASS_NAMES[t]}  pred={CLASS_NAMES[p]}\\n\"\n",
    "            f\"margin={margins[k]:.2f}\"\n",
    "        )\n",
    "        axes[row, 0].axis('off')\n",
    "        \n",
    "        # Show contribution maps for each class\n",
    "        for c in range(len(CLASS_NAMES)):\n",
    "            axes[row, 1+c].imshow(contribution_map(k, c), cmap='magma')\n",
    "            axes[row, 1+c].set_title(\n",
    "                f\"{CLASS_NAMES[c]}\\n\"\n",
    "                f\"cos={cos[c]:.2f}\\n\"\n",
    "                f\"score={scores[k,c]:.1f}\"\n",
    "            )\n",
    "            axes[row, 1+c].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_panel(hard, \"Hardest\")\n",
    "show_panel(easy, \"Easiest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ab7fd-4bf1-45e2-ae11-286abd58c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def avg_contrib_for_subset(mask, class_idx):\n",
    "    \"\"\"Compute average contribution heatmap for a subset of images.\"\"\"\n",
    "    idxs = np.where(mask)[0]\n",
    "    C = np.stack([contribution_map(i, class_idx) for i in idxs], axis=0)\n",
    "    return C.mean(axis=0) if len(C) else np.zeros((H, W))\n",
    "\n",
    "# Show average contributions for each true/predicted class combination\n",
    "for t_idx, t_name in enumerate(CLASS_NAMES):\n",
    "    for p_idx, p_name in enumerate(CLASS_NAMES):\n",
    "        mask = (y_true == t_idx) & (y_pred == p_idx)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        fig, axes = plt.subplots(1, len(CLASS_NAMES)+1, figsize=(12, 3))\n",
    "        \n",
    "        # Average input image\n",
    "        axes[0].imshow(np.mean(X[mask], axis=0), cmap='gray')\n",
    "        axes[0].set_title(f\"Avg IMG\\ntrue={t_name}, pred={p_name}\\nN={mask.sum()}\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Average contribution to each class\n",
    "        for c in range(len(CLASS_NAMES)):\n",
    "            axes[1+c].imshow(avg_contrib_for_subset(mask, c), cmap='magma')\n",
    "            axes[1+c].set_title(f\"Avg contrib → {CLASS_NAMES[c]}\")\n",
    "            axes[1+c].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b2264-4290-4fae-b6d3-f65596e49a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Score distributions for each class\n",
    "for c, name in enumerate(CLASS_NAMES):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.hist(scores[y_true==c, c], bins=20, alpha=0.8)\n",
    "    plt.title(f\"Score distribution for true {name} → class {name}\")\n",
    "    plt.xlabel(\"Raw score\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "# Margin distributions by true class\n",
    "plt.figure(figsize=(5, 3))\n",
    "for c, name in enumerate(CLASS_NAMES):\n",
    "    plt.hist(margins[y_true==c], bins=20, alpha=0.5, label=name)\n",
    "plt.title(\"Top-1 margin by true class\")\n",
    "plt.xlabel(\"margin (top - runner-up)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1db5c-229a-459b-b9e6-8e346c91de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Install pandas if needed\n",
    "!pip install pandas -q\n",
    "import pandas as pd\n",
    "\n",
    "# Get top-2 predictions for each image\n",
    "top2 = np.argpartition(-scores, 2, axis=1)[:, :2]\n",
    "top2_scores = np.take_along_axis(scores, top2, axis=1)\n",
    "\n",
    "# Build detailed results table\n",
    "rows = []\n",
    "for i, (a, b) in enumerate(top2):\n",
    "    s1, s2 = top2_scores[i, np.argsort(-top2_scores[i])]\n",
    "    a, b = top2[i, np.argsort(-top2_scores[i])]\n",
    "    rows.append({\n",
    "        \"file\": paths[i].name,\n",
    "        \"true\": CLASS_NAMES[y_true[i]],\n",
    "        \"pred\": CLASS_NAMES[y_pred[i]],\n",
    "        \"runner_up\": CLASS_NAMES[b],\n",
    "        \"pred_score\": scores[i, a],\n",
    "        \"runner_score\": scores[i, b],\n",
    "        \"margin\": scores[i, a] - scores[i, b]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"margin\")\n",
    "\n",
    "# Show most and least confident predictions\n",
    "display(df.head(10))  # Lowest margins (least confident)\n",
    "display(df.tail(10))  # Highest margins (most confident)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
